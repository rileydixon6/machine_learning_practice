{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rileydixon6/machine_learning_practice/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8RbnIjwHGoR",
        "outputId": "26e0368f-73fa-4e1f-90a8-95e686307464"
      },
      "source": [
        "# Load file data\n",
        "text = open(\"/content/doyle.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1737118 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XRnt0XUHUrq",
        "outputId": "8187f950-93b6-4728-9485-7841dd0fbc64"
      },
      "source": [
        "# Verify the first part of our data\n",
        "print(text[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\r\n",
            "\r\n",
            "Title: The Adventures of Sherlock Holmes\r\n",
            "\r\n",
            "Author: Arthur Conan Doyle\r\n",
            "\r\n",
            "Release Date: November 29, 2002 [EBook #1661]\r\n",
            "Last Updated: May 20, 2019\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "Character set encoding:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SLd7l0HP1Po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fde8979-d986-496c-87db-0b168032f677"
      },
      "source": [
        "# Now we'll get a list of the unique characters in the file. This will form the\n",
        "# vocabulary of our network. There may be some characters we want to remove from this \n",
        "# set as we refine the network.\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98 unique characters\n",
            "['\\t', '\\n', '\\r', ' ', '!', '#', '&', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'º', '½', 'à', 'â', 'æ', 'è', 'é', 'î', 'ô', 'œ', '—', '‘', '’', '“', '”', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtjOxL7wQibb"
      },
      "source": [
        "# Encoding encode these characters into numbers so we can use them\n",
        "# with our neural network, then we'll create some mappings between the characters\n",
        "# and their numeric representations\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "# Here's a little helper function that we can use to turn a sequence of ids\n",
        "# back into a string:\n",
        "# turn them into a string:\n",
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52bkemreRw8q",
        "outputId": "7920e7a7-83c5-410c-a131-ecd674c2ec62"
      },
      "source": [
        "# Now we'll verify that they work, by getting the code for \"A\", and then looking\n",
        "# that up in reverse\n",
        "testids = ids_from_chars([\"T\", \"r\", \"u\", \"t\", \"h\"])\n",
        "testids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([47, 74, 77, 76, 64])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGUnSHjtD_IJ",
        "outputId": "6c7bb288-be3a-49a1-b514-4cb75e417341"
      },
      "source": [
        "chars_from_ids(testids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'T', b'r', b'u', b't', b'h'], dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8rghkpLLLjL5",
        "outputId": "bd1feae6-df8c-411f-9123-f095d8e5a5d8"
      },
      "source": [
        "testString = text_from_ids( testids )\n",
        "testString"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Truth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PLJWOg2P_fE",
        "outputId": "1cacbfa3-b322-44d2-8b92-9e2b8444ccca"
      },
      "source": [
        "# Create a stream of encoded integers from our text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1737118,), dtype=int64, numpy=array([99,  4,  3, ..., 79, 70, 14])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nBqVY6pFpZs"
      },
      "source": [
        "# Convert that into a tensorflow dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fr28CJxUBtG"
      },
      "source": [
        "# Batch these sequences up into chunks for our training\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Call the function for every sequence in our list to create a new dataset\n",
        "# of input->target pairs\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poNVukmsUFkq",
        "outputId": "e0e05e53-ca97-4af2-9808-63a52f1768fc"
      },
      "source": [
        "# Verify our sequences\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example))\n",
        "    print(\"--------\")\n",
        "    print(\"Target: \", text_from_ids(target_example))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  ﻿\r\n",
            "\r\n",
            "Title: The Adventures of Sherlock Holmes\r\n",
            "\r\n",
            "Author: Arthur Conan Doyle\r\n",
            "\r\n",
            "Release Date: Novembe\n",
            "--------\n",
            "Target:  \r\n",
            "\r\n",
            "Title: The Adventures of Sherlock Holmes\r\n",
            "\r\n",
            "Author: Arthur Conan Doyle\r\n",
            "\r\n",
            "Release Date: November\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDdr6xfZYa0o",
        "outputId": "1ad3de7a-dbc1-4d9d-d280-c6a81cf4fe8a"
      },
      "source": [
        "# Randomize the sequences so that we don't just memorize the books\n",
        "# in the order they were written, then build a new streaming dataset from that.\n",
        "# Using a streaming dataset allows us to pass the data to our network bit by bit,\n",
        "# rather than keeping it all in memory. We'll set it to figure out how much data\n",
        "# to prefetch in the background.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj4uh9y-Y9mx"
      },
      "source": [
        "# Create our custom model. Given a sequence of characters, this\n",
        "# model's job is to predict what character should come next.\n",
        "class DoyleTextModel(tf.keras.Model):\n",
        "\n",
        "  # This is our class constructor method, it will be executed when\n",
        "  # we first create an instance of the class \n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "\n",
        "    # Our model will have three layers:\n",
        "    \n",
        "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
        "    #    a vector of values suitable for a neural network\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
        "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
        "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
        "    #    then consider trying out LSTM instead (or in addition to!)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    # 3. Our output layer that will give us a set of probabilities for each\n",
        "    #    character in our vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # This function will be executed for each epoch of our training. Here\n",
        "  # we will manually feed information from one layer of our network to the \n",
        "  # next.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
        "    #    training or predicting\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    # 2. If we don't have any state in memory yet, get the initial random state\n",
        "    #    from our GRUI layer.\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    \n",
        "    # 3. Now, feed the vectorized input along with the current state of memory\n",
        "    #    into the gru layer.\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "    # 4. Finally, pass the results on to the dense layer\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # 5. Return the results\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA2C6pxZc4De"
      },
      "source": [
        "# Create an instance of our model\n",
        "vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = DoyleTextModel(vocab_size, embedding_dim, rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67kN7YAdfSf",
        "outputId": "bb3a9a6e-1aa5-4ff8-e10a-0554a9bc56c4"
      },
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 100) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGL8gCWdsiu",
        "outputId": "d8fb438a-4d34-4ee6-c6cc-37e6a82d3e9e"
      },
      "source": [
        "# Now let's view the model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"doyle_text_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  25600     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  102500    \n",
            "=================================================================\n",
            "Total params: 4,066,404\n",
            "Trainable params: 4,066,404\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vOxc7CkaGQB",
        "outputId": "c5f49af9-8406-47fe-891d-d0476a4ea4ef"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(dataset, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "268/268 [==============================] - 16s 51ms/step - loss: 2.9758\n",
            "Epoch 2/20\n",
            "268/268 [==============================] - 15s 51ms/step - loss: 1.7003\n",
            "Epoch 3/20\n",
            "268/268 [==============================] - 15s 51ms/step - loss: 1.3906\n",
            "Epoch 4/20\n",
            "268/268 [==============================] - 15s 52ms/step - loss: 1.2361\n",
            "Epoch 5/20\n",
            "268/268 [==============================] - 15s 52ms/step - loss: 1.1440\n",
            "Epoch 6/20\n",
            "268/268 [==============================] - 15s 52ms/step - loss: 1.0787\n",
            "Epoch 7/20\n",
            "268/268 [==============================] - 15s 53ms/step - loss: 1.0178\n",
            "Epoch 8/20\n",
            "268/268 [==============================] - 15s 53ms/step - loss: 0.9644\n",
            "Epoch 9/20\n",
            "268/268 [==============================] - 15s 54ms/step - loss: 0.9044\n",
            "Epoch 10/20\n",
            "268/268 [==============================] - 15s 54ms/step - loss: 0.8477\n",
            "Epoch 11/20\n",
            "268/268 [==============================] - 15s 54ms/step - loss: 0.7933\n",
            "Epoch 12/20\n",
            "268/268 [==============================] - 16s 54ms/step - loss: 0.7336\n",
            "Epoch 13/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.6827\n",
            "Epoch 14/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.6373\n",
            "Epoch 15/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.5949\n",
            "Epoch 16/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.5595\n",
            "Epoch 17/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.5320\n",
            "Epoch 18/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.5099\n",
            "Epoch 19/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.4967\n",
            "Epoch 20/20\n",
            "268/268 [==============================] - 16s 55ms/step - loss: 0.4774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3lhlyfwcqIN"
      },
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    \n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return chars_from_ids(predicted_ids), states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSUghgUFc6ba",
        "outputId": "4c201027-498f-4109-909c-012b5a1bc091"
      },
      "source": [
        "# Create an instance of the character generator\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "# as its starting text\n",
        "states = None\n",
        "next_char = tf.constant(['Chapter 1'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the results formatted.\n",
        "print(result[0].numpy().decode('utf-8'))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 187 a\r\n",
            "      day or two things in front of my loper, I was too seriously could on\r\n",
            "      the matter, then; and the detection is did all the information in\r\n",
            "      the edge of the father and over his extreme knowledges than ill usually\r\n",
            "      formed to change in his mind, and that he only deeply\r\n",
            "      elbow, and if he were realised, therefore, to cut your hair, Holmes, who\r\n",
            "knew her in your power of regret,” said Holmes as we\r\n",
            "      answered him to pain with him which he clarped down, he remembered the\r\n",
            "story was ever searched, but there was something in his eyes claimbed most head by a sharp rattle\r\n",
            "      rather susterious imperturbably. And this he remained with a\r\n",
            "      prompt and his exercise, rather stone up, the three\r\n",
            "overcoat, we should have the two days of the question. You will find it\r\n",
            "      two o’clock when she knocked without a word be\r\n",
            "driver here without petriting in rusing the shoulder. A\r\n",
            "      hand, keen-faced young fellow, steeping in the same\r\n",
            "      dight, however, wa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCnuBqmH5TRi"
      },
      "source": [
        "class CustomTraining(DoyleTextModel):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(vocab_size, embedding_dim, rnn_units)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jekhdeO85sJ-"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A4_7f595w2g"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9QZTvNl50Ug",
        "outputId": "86fed6bb-efa7-468f-c8ea-e2229e246d37"
      },
      "source": [
        "model.fit(dataset, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "268/268 [==============================] - 31s 106ms/step - loss: 2.3762\n",
            "Epoch 2/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 1.5951\n",
            "Epoch 3/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 1.3373\n",
            "Epoch 4/20\n",
            "268/268 [==============================] - 29s 105ms/step - loss: 1.2101\n",
            "Epoch 5/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 1.1294\n",
            "Epoch 6/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 1.0664\n",
            "Epoch 7/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 1.0110\n",
            "Epoch 8/20\n",
            "268/268 [==============================] - 29s 105ms/step - loss: 0.9564\n",
            "Epoch 9/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 0.9024\n",
            "Epoch 10/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 0.8464\n",
            "Epoch 11/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 0.7905\n",
            "Epoch 12/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 0.7369\n",
            "Epoch 13/20\n",
            "268/268 [==============================] - 29s 105ms/step - loss: 0.6860\n",
            "Epoch 14/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 0.6390\n",
            "Epoch 15/20\n",
            "268/268 [==============================] - 29s 105ms/step - loss: 0.5989\n",
            "Epoch 16/20\n",
            "268/268 [==============================] - 29s 105ms/step - loss: 0.5658\n",
            "Epoch 17/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 0.5378\n",
            "Epoch 18/20\n",
            "268/268 [==============================] - 29s 105ms/step - loss: 0.5150\n",
            "Epoch 19/20\n",
            "268/268 [==============================] - 29s 106ms/step - loss: 0.4979\n",
            "Epoch 20/20\n",
            "268/268 [==============================] - 29s 105ms/step - loss: 0.4846\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1f71ad3b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYVsvSfwUNW5",
        "outputId": "5eafb689-220f-4696-8844-ff795c8c5554"
      },
      "source": [
        "# Create an instance of the character generator\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "# Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "# as its starting text\n",
        "states = None\n",
        "next_char = tf.constant(['Chapter 1'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the results formatted.\n",
        "print(result[0].numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 13_s_, and now I beg that you\r\n",
            "      will not find their stepfather for three years ago than no easy. I remember, and I\r\n",
            "      made off with him. But Alec Fairbairn whose step\r\n",
            "      How long they would not think there anywhere at the breakfast was clear, and\r\n",
            "      made my secret with a persettern, and, walking towards the bell, and,\r\n",
            "      married, without a careful examination mad with a\r\n",
            "chill of room, but I call it for handy demonstressed by my\r\n",
            "      promise, that he may have been terribly impatiently asking him from\r\n",
            "      my dressing-gown was to find am some one about that Sides and\r\n",
            "      weight on with her. So much the woman thought\r\n",
            "what was the business over, and nervous was a fool and swiftly affection. We had\r\n",
            "      really two entwers—of the landlory’s office, and\r\n",
            "      puzzled now of the wall as they had fallen our resist and\r\n",
            "      learned. In left by the fire as we entered.\r\n",
            "\r\n",
            "      “It was all the wife, and my wife by\r\n",
            "other woman, black face, which purchased it with \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}