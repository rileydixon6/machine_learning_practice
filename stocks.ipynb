{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "import pandas as pd\n",
    "from alpaca_trade_api.rest import REST, TimeFrame\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senior Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tradeapi.REST('AKP5ZTIF1EZY9IFOX8PW', '8tz0hS5SLcAGYQNR5yW6XoGOXIQ4Ehb6q270ELNK', 'https://api.alpaca.markets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in Data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'REST' object has no attribute 'get_barset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m datasets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m symbols:\n\u001b[1;32m----> 9\u001b[0m     stock_data \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_barset\u001b[49m(ticker, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, limit\u001b[38;5;241m=\u001b[39minstances)\n\u001b[0;32m     11\u001b[0m     stock_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(stock_data\u001b[38;5;241m.\u001b[39mdf)\n\u001b[0;32m     13\u001b[0m     stock_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m stock_data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdroplevel()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'REST' object has no attribute 'get_barset'"
     ]
    }
   ],
   "source": [
    "instances = 1000\n",
    "\n",
    "symbols = ['GOOG', 'AAPL', 'ROKU']\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for ticker in symbols:\n",
    "\n",
    "    stock_data = api.get_barset(ticker, 'day', limit=instances)\n",
    "\n",
    "    stock_data = pd.DataFrame(stock_data.df)\n",
    "\n",
    "    stock_data.columns = stock_data.columns.droplevel()\n",
    "\n",
    "    stock_data = stock_data.reset_index()\n",
    "\n",
    "    stock_data['symbol'] = ticker\n",
    "\n",
    "    datasets.append(stock_data)\n",
    "    \n",
    "\n",
    "pd.set_option(\"display.max_rows\", 10, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation:\n",
    "\n",
    "For feature creation I must first decide what I want the data table to look like. I would like to create a big datatable with data from multiple stock tickers and each row represents one stock for one day. Because the target variable is the price for the next day I would also like to create features that revolve around the stock conditions for the current day. This mainly revolves around the price and volume.\n",
    "\n",
    "\n",
    "Here, instead of having a single timestamp, I would like to separate it into several columns for day, month, and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riley\\AppData\\Local\\Temp/ipykernel_18180/191631716.py:7: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df['week'] = df[\"time\"].dt.week\n"
     ]
    }
   ],
   "source": [
    "for df in datasets:\n",
    "\n",
    "    df[\"total_high\"] = df[\"high\"].max()\n",
    "\n",
    "    df['day'] = df[\"time\"].dt.day\n",
    "\n",
    "    df['week'] = df[\"time\"].dt.week\n",
    "\n",
    "    df['month'] = df[\"time\"].dt.month\n",
    "\n",
    "    df['year'] = df[\"time\"].dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the weekly, monthly, and yearly maximums for price and volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in datasets:\n",
    "\n",
    "    df['week_high'] = df.groupby(['week','month','year'])['high'].transform('max')\n",
    "\n",
    "    df['month_high'] = df.groupby(['month', 'year'])['high'].transform('max')\n",
    "\n",
    "    df['year_high'] = df.groupby(df['year'])['high'].transform('max')\n",
    "\n",
    "    df['week_max_volume'] = df.groupby(['week','month','year'])['volume'].transform('max')\n",
    "\n",
    "    df['month_max_volume'] = df.groupby(['month','year'])['volume'].transform('max')\n",
    "\n",
    "    df['year_max_volume'] = df.groupby(df['year'])['volume'].transform('max')\n",
    "\n",
    "    df[\"volume_median\"] = df[\"volume\"].median()\n",
    "\n",
    "    df[\"day_middle\"] = (df[\"high\"] + df[\"low\"]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where most of the features we will use are generated. Including consecutive days a stock ticker has been negative or positive, rolling averages and current price relative to week, month, or year maximums or the rolling average. Because we will be using multiple tickers in the dataset the end goal for my features is that there is a reasonable range in which all columns for all stocks can fit in, this is why you'll see a lot of percentage values rather than the actual price value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price\n",
    "\n",
    "for df in datasets:\n",
    "\n",
    "    df['daily_change'] = df.close - df.close.shift()\n",
    "\n",
    "    df['daily_change_percentage'] = (df.daily_change / df.close.shift()) * 100\n",
    "\n",
    "    df['daily_positive_change'] = df.daily_change > 1\n",
    "\n",
    "    df['daily_negative_change'] = df.daily_change < 1\n",
    "\n",
    "    df[\"consecutive_days\"] = df.daily_positive_change.groupby((df.daily_positive_change != df.daily_positive_change.shift()).cumsum()).cumcount() + 1\n",
    "\n",
    "    df['consecutive_days_positive'] = df.apply(lambda x: x[\"consecutive_days\"] if x['daily_positive_change'] == True else 0, axis = 1)\n",
    "\n",
    "    df['consecutive_days_negative'] = df.apply(lambda x: x[\"consecutive_days\"] if x['daily_negative_change'] == True else 0, axis = 1)\n",
    "\n",
    "    df[\"median_daily_change\"] = df.daily_change.median()\n",
    "\n",
    "    df[\"median_loss\"] = df[\"daily_change\"].where(df['daily_change'] < 0).median()\n",
    "\n",
    "    df[\"median_gain\"] = df[\"daily_change\"].where(df['daily_change'] > 0).median()\n",
    "\n",
    "    df[\"30_day_rolling_average_percentage_change\"] = df[\"daily_change_percentage\"].rolling(30).median()\n",
    "\n",
    "    df[\"30_day_rolling_median_price\"] = df[\"day_middle\"].rolling(30).median()\n",
    "\n",
    "    df[\"price_relative_to_week_high\"] = (df.day_middle / df.week_high) * 100\n",
    "\n",
    "    df[\"price_relative_to_month_high\"] = (df.day_middle / df.month_high) * 100\n",
    "\n",
    "    df[\"price_relative_to_year_high\"] = (df.day_middle / df.year_high) * 100\n",
    "\n",
    "    df[\"price_relative_to_all_time_high\"] = (df.day_middle / df.total_high) * 100\n",
    "\n",
    "    df[\"price_relative_to_30_rolling_median\"] = (df.day_middle / df[\"30_day_rolling_median_price\"]) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I completed the same steps as above but for volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#volume\n",
    "\n",
    "for df in datasets:\n",
    "\n",
    "    df['daily_volume_change'] = df.volume - df.volume.shift()\n",
    "\n",
    "    df['daily_volume_change_percentage'] = (df.daily_volume_change / df.volume.shift()) * 100\n",
    "\n",
    "    df['daily_volume_positive_change'] = df.daily_volume_change > 1\n",
    "\n",
    "    df['daily_volume_negative_change'] = df.daily_volume_change < 1\n",
    "\n",
    "    df[\"consecutive_days_volume\"] = df.daily_volume_positive_change.groupby((df.daily_volume_positive_change != df.daily_volume_positive_change.shift()).cumsum()).cumcount() + 1\n",
    "\n",
    "    df['consecutive_days_volume_positive'] = df.apply(lambda x: x[\"consecutive_days_volume\"] if x['daily_volume_positive_change'] == True else 0, axis = 1)\n",
    "\n",
    "    df['consecutive_days_volume_negative'] = df.apply(lambda x: x[\"consecutive_days_volume\"] if x['daily_volume_negative_change'] == True else 0, axis = 1)\n",
    "\n",
    "    df[\"median_daily_volume_change\"] = df.daily_volume_change.median()\n",
    "\n",
    "    df[\"median_volume_loss\"] = df[\"daily_volume_change\"].where(df['daily_volume_change'] < 0).median()\n",
    "\n",
    "    df[\"median_volume_gain\"] = df[\"daily_volume_change\"].where(df['daily_volume_change'] > 0).median()\n",
    "\n",
    "    df[\"30_day_rolling_volume_average_percentage\"] = df[\"daily_volume_change_percentage\"].rolling(30).median()\n",
    "\n",
    "    df[\"30_day_rolling_median_volume\"] = df[\"volume\"].rolling(30).median()\n",
    "\n",
    "    df[\"volume_relative_to_week_max_volume\"] = (df.volume / df.week_max_volume) * 100\n",
    "\n",
    "    df[\"price_relative_to_month_max_volume\"] = (df.volume / df.month_max_volume) * 100\n",
    "\n",
    "    df[\"price_relative_to_year_max_volume\"] = (df.volume / df.year_max_volume) * 100\n",
    "\n",
    "    df[\"current_volume_relative_to_30_rolling_median\"] = (df.volume / df[\"30_day_rolling_median_volume\"]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I felt it would be best to one-hot-encode my features regarding consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1356,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatables = []\n",
    "\n",
    "for df in datasets:\n",
    "\n",
    "    one_hot_encoded = pd.get_dummies(df, columns=['consecutive_days_positive', 'consecutive_days_negative', 'consecutive_days_volume_positive', 'consecutive_days_volume_negative'])\n",
    "\n",
    "    dropped = one_hot_encoded.drop(columns=['consecutive_days', 'consecutive_days_volume', 'daily_negative_change', 'daily_volume_negative_change', 'time', 'daily_positive_change'])\n",
    "\n",
    "    datatables.append(dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Different Data Tables for Models\n",
    "\n",
    "Now I will begin to create three separate data tables based off of the data organized thus far. One table will predict if the next day stock price will be higher or lower(classification), one table will predict the next day percentage rise(regression), and the third table will predict the next day price percentage rise binned. I wanted these seperate tables to see which one works better in the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First data table (classification). My target variable is simply whether the next day price change will be higher or lower. In this step(for all tables), we will be dropping several columns previously created that may skew data results to certain stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_or_no = []\n",
    "\n",
    "#get target\n",
    "for df in datatables:\n",
    "\n",
    "  df[\"is_next_day_positive\"] = df.daily_change_percentage.shift(-1) > 1\n",
    "\n",
    "  dropped = df.drop(columns=['open', 'high', 'low', 'close', 'volume','symbol', 'total_high', 'week_high', 'month_high', 'year_high', 'week_max_volume', 'month_max_volume',\n",
    "    'year_max_volume', 'volume_median', 'day_middle', 'daily_change', 'median_daily_change', '30_day_rolling_median_price', '30_day_rolling_median_volume','daily_volume_change', 'median_loss', 'median_gain', 'median_daily_volume_change', 'median_volume_loss',\n",
    "    'median_volume_gain', 'daily_volume_positive_change'])\n",
    "\n",
    "#delete first 30 rows(too many null values)\n",
    "  less_rows = dropped.iloc[30: , :]\n",
    "\n",
    "  data = less_rows.reset_index(drop = True)\n",
    "\n",
    "  last_row_dropped = data = data[:-1]\n",
    "\n",
    "  yes_or_no.append(last_row_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_or_no_final = pd.concat(yes_or_no)\n",
    "\n",
    "yes_or_no_final = yes_or_no_final.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second data table (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_prediction = []\n",
    "\n",
    "#get target\n",
    "for df in datatables:\n",
    "\n",
    "  df[\"is_next_day_positive\"] = df.daily_change_percentage.shift(-1)\n",
    "\n",
    "  dropped = df.drop(columns=['open', 'high', 'low', 'close', 'volume','symbol', 'total_high', 'week_high', 'month_high', 'year_high', 'week_max_volume', 'month_max_volume',\n",
    "    'year_max_volume', 'volume_median', 'day_middle', 'daily_change', 'median_daily_change', '30_day_rolling_median_price', '30_day_rolling_median_volume','daily_volume_change', 'median_loss', 'median_gain', 'median_daily_volume_change', 'median_volume_loss',\n",
    "    'median_volume_gain', 'daily_volume_positive_change'])\n",
    "\n",
    "  less_rows = dropped.iloc[30: , :]\n",
    "\n",
    "  data = less_rows.reset_index(drop = True)\n",
    "\n",
    "  last_row_dropped = data = data[:-1]\n",
    "\n",
    "  number_prediction.append(last_row_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_data = number_prediction\n",
    "\n",
    "number_prediction = pd.concat(number_prediction)\n",
    "\n",
    "number_prediction = number_prediction.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third data table(classification binned).\n",
    "I ran into some trouble with this one and the code is sloppy but it should work now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1361,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_data = pd.concat(binned_data)\n",
    "\n",
    "binned_data = binned_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bins\n",
    "\n",
    "columns_to_bin = [\"daily_change_percentage\", '30_day_rolling_average_percentage_change', 'price_relative_to_week_high', \n",
    "    'price_relative_to_month_high', 'price_relative_to_year_high', 'price_relative_to_all_time_high', 'price_relative_to_30_rolling_median', 'daily_volume_change_percentage', \n",
    "    '30_day_rolling_volume_average_percentage', 'volume_relative_to_week_max_volume', 'price_relative_to_month_max_volume', 'price_relative_to_year_max_volume', \n",
    "    'current_volume_relative_to_30_rolling_median', \"is_next_day_positive\"]\n",
    "\n",
    "bins = 20\n",
    "\n",
    "for column in columns_to_bin:\n",
    "\n",
    "    #df[column] = pd.qcut(df[column].rank(method='first'), bins)\n",
    "    binned_data[column] = pd.qcut(binned_data[column], bins, duplicates='drop')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'yes_or_no.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18180/1490539244.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myes_or_no_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'yes_or_no.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbinned_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'binned_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnumber_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number_prediction.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'yes_or_no.csv'"
     ]
    }
   ],
   "source": [
    "yes_or_no_final.to_csv('yes_or_no.csv')\n",
    "binned_data.to_csv('binned_data.csv')\n",
    "number_prediction.to_csv('number_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Models\n",
    "I decided to try a Random Forest, XGBoost, and Neural Networks. All of the models are working, but I haven't quite found the parameter I want yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, separate the data from each table to show the x and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_yes_or_no = yes_or_no_final.drop(columns = ['is_next_day_positive'])\n",
    "y_yes_or_no = yes_or_no_final['is_next_day_positive']\n",
    "\n",
    "X_binned = binned_data.drop(columns = ['is_next_day_positive'])\n",
    "y_binned = binned_data['is_next_day_positive']\n",
    "\n",
    "X = number_prediction.drop(columns = ['is_next_day_positive'])\n",
    "y = number_prediction['is_next_day_positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher or Lower Classification Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>is_next_day_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predictions  is_next_day_positive\n",
       "0          False                 False\n",
       "1          False                 False\n",
       "2          False                 False\n",
       "3          False                 False\n",
       "4          False                 False\n",
       "..           ...                   ...\n",
       "868        False                 False\n",
       "869        False                  True\n",
       "870        False                  True\n",
       "871        False                 False\n",
       "872        False                 False\n",
       "\n",
       "[873 rows x 2 columns]"
      ]
     },
     "execution_count": 1365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_yes_or_no, y_yes_or_no, test_size = .3, random_state = 1)\n",
    "\n",
    "forest_model = RandomForestClassifier(max_depth = 2, random_state=1)\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = forest_model.predict(X_test)\n",
    "\n",
    "predictions = pd.DataFrame(y_pred)\n",
    "\n",
    "predictions = predictions.rename(columns={0: \"predictions\"})\n",
    "\n",
    "actual = pd.DataFrame(y_test)\n",
    "\n",
    "actual = actual.reset_index(drop = True)\n",
    "\n",
    "results = pd.concat([predictions, actual], axis=1)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, cross validation is used to check for overfitting. Basically we are testing the model with five different, random sections of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69 accuracy with a standard deviation of 0.00\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(forest_model, X_yes_or_no, y_yes_or_no, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show what features were most important in finding the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-f7d42f51483e4d11990726fc0afa1415\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f7d42f51483e4d11990726fc0afa1415\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f7d42f51483e4d11990726fc0afa1415\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-e2e6ebb4a580d1036fc49b8d822a7884\"}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"values\"}, \"y\": {\"type\": \"nominal\", \"field\": \"features\", \"sort\": \"-x\"}}, \"title\": \"Most Important Features\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-e2e6ebb4a580d1036fc49b8d822a7884\": [{\"values\": 0.03211656343289946, \"features\": \"day\"}, {\"values\": 0.07744303128152677, \"features\": \"daily_change_percentage\"}, {\"values\": 0.042179768351603676, \"features\": \"30_day_rolling_average_percentage_change\"}, {\"values\": 0.20698231317714177, \"features\": \"price_relative_to_week_high\"}, {\"values\": 0.13518016346618988, \"features\": \"price_relative_to_month_high\"}, {\"values\": 0.05784976506955185, \"features\": \"price_relative_to_year_high\"}, {\"values\": 0.10300200210189814, \"features\": \"price_relative_to_all_time_high\"}, {\"values\": 0.06018608442984886, \"features\": \"price_relative_to_30_rolling_median\"}, {\"values\": 0.028761453436673818, \"features\": \"daily_volume_change_percentage\"}, {\"values\": 0.048564442222801645, \"features\": \"30_day_rolling_volume_average_percentage\"}, {\"values\": 0.0512954557974894, \"features\": \"price_relative_to_month_max_volume\"}, {\"values\": 0.04087557235211005, \"features\": \"price_relative_to_year_max_volume\"}, {\"values\": 0.03335609351660955, \"features\": \"current_volume_relative_to_30_rolling_median\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 1368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dat = pd.DataFrame({\n",
    "    \"values\": forest_model.feature_importances_,\n",
    "    \"features\": X_train.columns\n",
    "    }) \n",
    "alt.Chart(feature_dat.query('values > .02'),title = \"Most Important Features\").encode(\n",
    "    alt.X('values'), \n",
    "    alt.Y('features', sort = \"-x\")).mark_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the predictions vs the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riley\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x16a02144a60>"
      ]
     },
     "execution_count": 1369,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAEGCAYAAADscbcsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdwElEQVR4nO3deZwdZZ3v8c83nQ4hCSSELIQkQJAIZnCIISKLMiEgm94JziCgKIyXGVxwuaJecYZRrjqMekUQETSAl6CyIxIcIGCAy3IJJGAk7IkhZCFbhxDMAul0/+4f9TQ5hO5zqpM+XX26v29e9TpVTz2n6tdp8svz1FP1lCICMzMrr1fRAZiZ1QInSzOzHJwszcxycLI0M8vBydLMLIfeRQdQDUMG18U+o+uLDsPa4cWn+hUdgrXTX1nbEBFDd+QYxx3VP9a82pSr7hNPvTkjIo7fkfPtiG6ZLPcZXc/jM0YXHYa1w3F7ji86BGunP8YtL+/oMRpebeKxGaNy1a0f8ZchO3q+HeFuuJkVKGiK5lxLJZIGSbpF0vOSnpN0mKTBku6VND997pbqStKlkhZIekrShErHd7I0s8IE0EzkWnL4KXB3RBwAHAQ8B5wHzIyIscDMtA1wAjA2LWcDV1Q6uJOlmRWqOed/5UgaCBwJXA0QEZsj4jVgCjAtVZsGnJTWpwDXRmYWMEjSiHLn6JbXLM2sNgRBY44udjJE0pyS7akRMTWtjwFWA/9H0kHAE8BXgOERsTzVWQEMT+sjgSUlx1qaypbTBidLMytMAE35utgADRExsY19vYEJwJci4jFJP2Vrlzs7V0RI2u7JMNwNN7NCddA1y6XA0oh4LG3fQpY8V7Z0r9PnqrR/GVB6y8yoVNYmJ0szK0wATRG5lrLHiVgBLJG0fyo6GngWmA6cmcrOBG5P69OBM9Ko+KHAupLueqvcDTezQuW+YlnZl4DfSuoDLAQ+Q9YgvEnSWcDLwCmp7p3AicACYGOqW5aTpZkVJoj2XLMsf6yIuUBr1zSPbqVuAOe05/hOlmZWmAhorJH5x50szaxAogkVHUQuTpZmVpgAmt2yNDOrzC1LM7MKspvSnSzNzMoKoDFq43ZvJ0szK0wgmmrk2RgnSzMrVHO4G25mVpavWZqZ5SKafM3SzKy8bKZ0J0szs7IixOaoKzqMXJwszaxQzb5maWZWXjbA4264mVkFHuAxM6vIAzxmZjk1+aZ0M7PyAtEYtZGGaiNKM+uWPMBjZpZDIHfDzczy8ACPmVkFEfjWITOzSrIBHj/uaGZWkQd4zMwqCOTJf83M8qiVlmVtRGlm3VL23vBeuZZKJC2SNE/SXElzUtlgSfdKmp8+d0vlknSppAWSnpI0odLxnSzNrECiKeeS01ERMT4iJqbt84CZETEWmJm2AU4AxqblbOCKSgd2sjSzwmSvwq3LtWynKcC0tD4NOKmk/NrIzAIGSRpR7kBOlmZWmAi1pxs+RNKckuXsbQ8H3CPpiZJ9wyNieVpfAQxP6yOBJSXfXZrK2uQBHjMrVDtuSm8o6V635oMRsUzSMOBeSc+X7oyIkBTbG6dblmZWmGw+S+VaKh4rYln6XAXcBhwCrGzpXqfPVan6MmB0yddHpbI2OVmaWYGymdLzLGWPIvWXtEvLOnAs8DQwHTgzVTsTuD2tTwfOSKPihwLrSrrrrXI33MwKk9061CE3pQ8HbpMEWV67LiLuljQbuEnSWcDLwCmp/p3AicACYCPwmUoncLI0s8J01LPhEbEQOKiV8jXA0a2UB3BOe87hZGlmhfIUbWZmFWRTtPnZcDOzijyRhplZBdmsQ+6Gm5mVlT3u6GRp22H9ujou/vpoFj3fFwnO/cliGpbX8+uL9mDJ/L5ceueLvPugTQA8/6d+/PQb2X21AXz6ays44oR1BUZvpSZOep3Pfe8V6noFd10/mJsuG175Sz2OW5ZIagLmlRSdFBGL2qi7PiIGVCuWWnLFt0cycdLr/PuVi2jcLN7c1IsBA5v49lWLuPSbo99Wd5/9N3HZ3S9Q1xvWrOzN54/Zn0M/vI46/xNYuF69gnMuXMa3TtuXhuX1/OzO+cyaMZDF8/sWHVqXk+fpnK6gmn+tNkXE+Coev9vZ8Hov5s3qz9cvWQxAfZ+gvk8TAwY2tVq/b7+tj7k2vtkL1cb/cz3C/u/byCuL+rBi8U4APHD7IA47bp2T5TZqaTS809q/kgZIminpyTRB55RW6oyQ9GCavPNpSR9K5cdKejR992ZJ3bIVumLxTgzcfQsXfXUvvvDhd3Px10bzxsbyv6Lnn+zHv0zan89O3p8v/3CpW5VdxO57NLL6lT5vbTcsr2fIiMYCI+q6Omry32qrZgQ7p6Q3V9JtwBvAxyJiAnAUcJH0jrbQJ4EZqUV6EDBX0hDgfOCY9N05wLnbnkzS2S1TN61e03pLrKtraoIF8/rx0TMauPzeF+nbr5kbLxtW9jsHTNjIlQ+8wM/uepEbfjaMzW/Uxr/SZrD1HTx5lqJ1WjdcUj1woaQjgWayueOGk80x12I28KtU9/cRMVfS3wHjgEdSbu0DPLrtySJiKjAVYOJBfbd7GqYiDRnRyNARjRwwYSMAH/zoa9xUIVm22Gvsm+zcv5lFL/R9awDIirNmRT1D99z81vaQEY00LK8vMKKuKYAtXaDVmEdnRnk6MBQ4OCXRlcDbLuBExIPAkWRTJV0j6QxAwL1pqvjxETEuIs7qxLg7zeBhWxiy52aWLMiuc819aBf2Gvtmm/VXLO5D05ZsfeXSepYs6MvwUZvbrG+d54W5/Rg5ZjPDR79J7/pmJk15jVn3DCw6rC6pVrrhnXmFayCwKiIaJR0F7L1tBUl7A0sj4kpJOwETgP8Afi5pv4hYkKZfGhkRL3Zi7J3mnO8v44df3JstjWKPvTbztYsX88hdA7n8/JGsW9Obf//0vrzrbzZx4fULefrx/tx42Rh6985GX7904VIG7l6blyC6m+Ym8fN/G8mF1y2kVx3cc8NgXn7Rgzvv0EW62Hl0ZrL8LXCHpHlk1x2fb6XOJOAbkhqB9cAZEbFa0j8B16cECtk1zG6ZLN914CYuu/vtP9oRJ6xr9f7JY05eyzEnr+2s0KydZt+3K7Pv27XoMLq0lsl/a0HVkuW2901GRANwWLm6ETGNrS8XKt1/H/D+KoRpZgVzy9LMrIIOnPy36pwszawwgdjSXPzgTR5OlmZWqB5/zdLMrKJwN9zMrCJfszQzy8nJ0sysgkA0eYDHzKwyD/CYmVUQHuAxM8snnCzNzCrxRBpmZrnUSsuyNoahzKxbioCmZuVa8pBUJ+lPkv6QtsdIekzSAkk3SuqTyndK2wvS/n0qHdvJ0swK1YxyLTl9BXiuZPuHwMURsR+wFmiZOPwsYG0qvzjVK8vJ0swKE2Td8DxLJZJGAR8BrkrbAiYDt6Qq04CT0voUtk4HeQtwdCvvBHsbX7M0swK1a4BniKQ5JdtT07u3WlwC/E9gl7S9O/BaRKSXr7CU7N1fpM8lABGxRdK6VL+hrZM7WZpZoSL/6wUbImJiazskfZTstTVPSJrUMZG9nZOlmRWqg0bDjwD+XtKJZC9C3BX4KTBIUu/UuhxF9jJE0udoYKmk3mTvCFtT7gS+ZmlmhclGw3vlWsofJ74VEaMiYh/gNOC+iDgduB84OVU7E7g9rU9P26T990WUb+M6WZpZoSLyLdvpm8C5khaQXZO8OpVfDeyeys8Fzqt0IHfDzaxQHX1TekQ8ADyQ1hcCh7RS5w3g4+05rpOlmRUmyHdbUFfgZGlmhdr+HnbncrI0s+IERM5HGYvmZGlmhXI33Mwshx0Y6e5UbSZLST+jzOWEiPhyVSIysx6j5dnwWlCuZTmnzD4zsx0XQK0ny4iYVrotqV9EbKx+SGbWk9RKN7ziEzySDpP0LPB82j5I0uVVj8zMegARzfmWouV53PES4DjSQ+YR8WfgyCrGZGY9SeRcCpZrNDwilmwzL2ZTdcIxsx4luscAT4slkg4HQlI975y23cxs+3WBVmMeebrhnwPOIZtZ+BVgfNo2M+sAyrkUq2LLMiIagNM7IRYz64maiw4gnzyj4ftKukPSakmrJN0uad/OCM7MurmW+yzzLAXL0w2/DrgJGAHsCdwMXF/NoMys56jy5L8dJk+y7BcRv46ILWn5Ddk7LszMdlyt3zokaXBavUvSecANZCGfCtzZCbGZWU/QBbrYeZQb4HmCLDm2/CSfLdkXwLeqFZSZ9RzqAq3GPMo9Gz6mMwMxsx4oBF3gUcY8cj3BI+lAYBwl1yoj4tpqBWVmPUittyxbSPoOMIksWd4JnAA8DDhZmtmOq5FkmWc0/GTgaGBFRHwGOAgYWNWozKznqPXR8BKbIqJZ0hZJuwKrgNFVjsvMeoLuMPlviTmSBgFXko2QrwcerWZQZtZz1PxoeIuI+EJa/YWku4FdI+Kp6oZlZj1GrSdLSRPK7YuIJ6sTkpn1JB3RspTUF3gQ2Iksr90SEd+RNIbsgZrdyXrGn46IzZJ2IhukPphsYvNTI2JRuXOUa1leVGZfAJPz/iCd7enVQxl3+RcqV7QuYzT/r+gQrCgdc83yTWByRKxP8+4+LOku4Fzg4oi4QdIvgLOAK9Ln2ojYT9JpwA/Jnk5sU7mb0o/qiJ/AzKxNHTTSHRFBNp4CUJ+WlkbdJ1P5NOACsmQ5Ja0D3AJcJknpOK3Kc+uQmVn15L91aIikOSXL2aWHkVQnaS7ZHTv3An8BXouILanKUrJJzEmfSwDS/nVkXfU25XqCx8ysWpR/8t+GiJjY1s6IaALGp7t3bgMO2OHgSrhlaWbF6uCb0iPiNeB+4DBgkKSWRuEoYFlaX0a6XzztH0h6g21b8syULkmfkvTttL2XpEPyh25m1jpF/qXscaShqUWJpJ2BD5O9WPF+sqcQAc4Ebk/r09M2af995a5XQr5u+OVkb8mYDHwX+CtwK/D+HN81MyuvY0bDRwDTJNWRNQJviog/SHoWuEHS94E/AVen+lcDv5a0AHgVOK3SCfIkyw9ExARJfwKIiLWS+mzHD2Nm9k4dMxr+FPC+VsoXAu/oCUfEG8DH23OOPMmyMWXrgKy5S828j83Murpu87gjcCnZyNIwSf9B1r8/v6pRmVnPEO0aDS9UnmfDfyvpCbJp2gScFBHPVT0yM+sZukvLUtJewEbgjtKyiFhczcDMrIfoLskS+C+2vrisLzAGeAH4myrGZWY9RLe5ZhkR7y3dTrMReZYKM+tR2v24Y0Q8KekD1QjGzHqg7tKylHRuyWYvYALwStUiMrOeozuNhgO7lKxvIbuGeWt1wjGzHqc7tCzTzei7RMTXOykeM+tBRDcY4JHUOyK2SDqiMwMysx6m1pMl8DjZ9cm5kqYDNwMbWnZGxO+qHJuZdXc5ZhTqKvJcs+xLNs/bZLbebxmAk6WZ7bhuMMAzLI2EP83WJNmiRv4tMLOurju0LOuAAbw9SbaokR/PzLq8Gskm5ZLl8oj4bqdFYmY9Twe93bEzlEuWHTJ9sZlZOd2hG350p0VhZj1XrSfLiHi1MwMxs56pOz3uaGZWHd3kmqWZWVWJ2hkccbI0s2K5ZWlmVll3GA03M6s+J0szswq62eS/ZmbV45almVlltXLNslfRAZhZDxc5lzIkjZZ0v6RnJT0j6SupfLCkeyXNT5+7pXJJulTSAklPpbfWluVkaWaFUuRbKtgCfC0ixgGHAudIGgecB8yMiLHAzLQNcAIwNi1nA1dUOoGTpZkVJ8gm/82zlDtMxPKIeDKt/xV4DhgJTAGmpWrTgJPS+hTg2sjMAgZJGlHuHL5maWaFaecLy4ZImlOyPTUipr7jmNI+wPuAx4DhEbE87VoBDE/rI4ElJV9bmsqW0wYnSzMrVv5k2RARE8tVkDSA7FXd/yMiXpe2PkwZESFt/3CSu+FmVihF5FoqHkeqJ0uUvy15oeLKlu51+lyVypcBo0u+PiqVtcnJ0syKk3ckvPJouICrgeci4iclu6YDZ6b1M4HbS8rPSKPihwLrSrrrrXI33MwK1UH3WR4BfBqYJ2luKvtX4AfATZLOAl4GTkn77gROBBYAG4HPVDqBk6WZFaojHneMiIdpe7a3d7z1ISICOKc953CyNLNi1cgTPE6WZlacfDecdwlOlmZWLCdLM7Py2nlTeqGcLM2sUGqujWzpZGlmxfHbHW177DFgPf959EyG7LyJAG56dhy/eepvATj9vfP4xIFP0xzi/768Nxc9ehgA/zLhSf7xPc/R1CwufPiDPLJkrwJ/Ais1cdLrfO57r1DXK7jr+sHcdNnwyl/qgTxTeglJu5NNjwSwB9AErE7bh0TE5s6Io6vb0ix+9MjhPNcwlH71m7nl47fw6JJR7L7zJibv8xIfu/EUGpvrGLzzRgDetdurnLDfAv7b9acxrP8Grv77Ozjxuk/QHH4wq2i9egXnXLiMb522Lw3L6/nZnfOZNWMgi+f3LTq0rscty60iYg0wHkDSBcD6iPhxy35JvSNiS2fE0pU1bOxPw8b+AGxs7MPCtbsxrP8GPj7uOa760wQam+sAeHVTPwAmj1nEXQv2o7G5jmV/3ZXF6wby3mGr+PPKPQr7GSyz//s28sqiPqxYvBMAD9w+iMOOW+dk2YpaGeAprAki6RpJv5D0GPAjSRdI+nrJ/qfTVEtI+pSkxyXNlfRLSXVFxd1Z9tzldd4zpIGnVg5nn0GvcfCIV7jhH29l2pTfc+CwbC6AYf03sGL9gLe+s3JDf4b331BUyFZi9z0aWf1Kn7e2G5bXM2REY4ERdVEBRORbClZ0f20UcHhEnNtWBUnvAU4FjoiI8WRd+NNbqXe2pDmS5jRtrO2E0a93Iz89bgb/+cgRbGjsQ52aGbjTm5x26z/w40cP4yfH3kPN9F3MKlBzvqVoRQ/w3BwRTRXqHA0cDMxOc9PtzNZplt6SJgGdCtB3z9E1m0l692rikuNn8If57+aPC/cFYMWGAdy7cF9AzFs1nOYQu/V9g1Ub+rPHgPVvfXd4/w2s3NC/oMit1JoV9Qzdc+ul+CEjGmlYXl9gRF1TLd1nWXTLsrQJuIW3x9NycUfAtIgYn5b9I+KCzgqwcwXfO+oBFq4dxLQ/H/RW6X0vjeGQkdlUe3sPfI36uibWvtGX+1/ahxP2W0B9ryZG7vI6ew98jXmrhhUVvJV4YW4/Ro7ZzPDRb9K7vplJU15j1j0Diw6r68nbBe8C3fCiW5alFgEfBUhvWhuTymcCt0u6OCJWSRoM7BIRLxcTZvVM2GMFU/Z/kRfWDOZ3p9wEwCWzPsDvnjuA70++n9tPvYHG5jr+deZkQCxYO5gZf3kXd3ziBpqaxfcf+pBHwruI5ibx838byYXXLaRXHdxzw2BeftGDO62plZZlV0qWt5JNxvkM2bszXgSIiGclnQ/cI6kX0Eg2tVK3S5ZPrhjBuMs/3+q+b/7xmFbLf/nEwfzyiYOrGZZtp9n37crs+3YtOoyuz8mydW11oSNiE3BsG/tuBG6sYlhmVhC3LM3MKgmgqTaypZOlmRXKLUszszy6wEh3Hk6WZlYotyzNzCrxFG1mZpUJkAd4zMwqk69ZmplV4G64mVkeXeO57zycLM2sULUyGu5ZF8ysWB0065CkX0laJenpkrLBku6VND997pbKJelSSQskPZUm7ynLydLMihPZaHieJYdrgOO3KTsPmBkRY8lmMDsvlZ8AjE3L2cAVlQ7uZGlmxYqcS6XDRDwIvLpN8RRgWlqfBpxUUn5tZGYBgySNKHd8X7M0s0K149ahIZLmlGxPTW9IKGd4RCxP6yuAlvcRjwSWlNRbmsqW0wYnSzMrVv5k2RARE7f/NBHS9g8nuRtuZsUJoDnnsn1WtnSv02fL+7uWAaNL6o1KZW1ysjSzwohAkW/ZTtOBM9P6mcDtJeVnpFHxQ4F1Jd31VrkbbmbFau6Y99xKuh6YRHZtcynwHeAHwE2SziJ7Fc0pqfqdwInAAmAj8JlKx3eyNLPitHTDO+JQEZ9oY9fRrdQNsnd55eZkaWaF8kQaZmZ5OFmamVXiiTTMzCrz2x3NzPLxNUszszycLM3MKgig2cnSzKwCD/CYmeXjZGlmVkEATR30CE+VOVmaWYECwsnSzKwyd8PNzCrwaLiZWU5uWZqZ5eBkaWZWQQQ0NRUdRS5OlmZWLLcszcxycLI0M6skPBpuZlZRQPimdDOzHPy4o5lZBREd9ircanOyNLNieYDHzKyycMvSzKwST/5rZlaZJ9IwM6ssgKiRxx17FR2AmfVgkSb/zbNUIOl4SS9IWiDpvI4O1S1LMytUdEA3XFId8HPgw8BSYLak6RHx7A4fPHHL0syK1TEty0OABRGxMCI2AzcAUzoyTEWNjES1h6TVwMtFx1ElQ4CGooOwdumuv7O9I2LojhxA0t1kfz559AXeKNmeGhFT03FOBo6PiH9O258GPhARX9yR+Ep1y274jv4CuzJJcyJiYtFxWH7+nbUtIo4vOoa83A03s+5gGTC6ZHtUKuswTpZm1h3MBsZKGiOpD3AaML0jT9Atu+Hd3NSiA7B28++syiJii6QvAjOAOuBXEfFMR56jWw7wmJl1NHfDzcxycLI0M8vB1ywLJqkJmFdSdFJELGqj7vqIGNApgVlZknYHZqbNPYAmYHXaPiTdGG3diK9ZFqw9CdDJsmuSdAGwPiJ+XFLWOyK2FBeVdTR3w7sYSQMkzZT0pKR5kt7xyJakEZIelDRX0tOSPpTKj5X0aPruzZKcWDuRpGsk/ULSY8CPJF0g6esl+5+WtE9a/5Skx9Pv8Jfp2Wbrwpwsi7dz+gszV9JtZI9zfSwiJgBHARdJ0jbf+SQwIyLGAwcBcyUNAc4HjknfnQOc22k/hbUYBRweEW3+2Ut6D3AqcET6HTYBp3dOeLa9fM2yeJvSXxgAJNUDF0o6EmgGRgLDgRUl35kN/CrV/X1EzJX0d8A44JGUW/sAj3bOj2Albo6IShM0Hg0cTDYzDsDOwKpqB2Y7xsmy6zkdGAocHBGNkhaRTSDwloh4MCXTjwDXSPoJsBa4NyI+0dkB29tsKFnfwtt7by2/RwHTIuJbnRaV7TB3w7uegcCqlCiPAvbetoKkvYGVEXElcBUwAZgFHCFpv1Snv6R3d2Lc9k6LyH43SJoAjEnlM4GTJQ1L+wan36l1YW5Zdj2/Be6QNI/suuPzrdSZBHxDUiOwHjgjIlZL+ifgekk7pXrnAy9WP2Rrw63AGZKeAR4j/S4i4llJ5wP3SOoFNALn0H2nFewWfOuQmVkO7oabmeXgZGlmloOTpZlZDk6WZmY5OFmameXgZNlDSWoqebb8Zkn9duBY16S36yHpKknjytSdJOnw7TjHovRIZ67ybeqsb+e53vZMtxk4WfZkmyJifEQcCGwGPle6U9J23YMbEf9c4cX2k4B2J0uzojlZGsBDwH6p1feQpOnAs5LqJP1vSbMlPSXpswDKXCbpBUl/BIa1HEjSA5ImpvXj0wxIf04zKe1DlpS/mlq1H5I0VNKt6RyzJR2Rvru7pHskPSPpKrJHBMuS9HtJT6TvnL3NvotT+UxJQ1PZuyTdnb7zkKQDOuRP07olP8HTw6UW5AnA3aloAnBgRLyUEs66iHh/eiroEUn3AO8D9iebuGM48Czwq22OOxS4EjgyHWtwRLwq6ReUzP0o6Trg4oh4WNJeZC+ceg/wHeDhiPiupI8AZ+X4cf57OsfOZJNU3BoRa4D+wJyI+Kqkb6djf5HsRWKfi4j5kj4AXA5M3o4/RusBnCx7rp0lzU3rDwFXk3WPH4+Il1L5scDftlyPJHtufSxwJHB9ml3nFUn3tXL8Q4EHW44VEa+2EccxwLiSWeh2VTYP55HAP6Tv/pektTl+pi9L+lhaH51iXUM2e9ONqfw3wO/SOQ4Hbi45906YtcHJsud629RwAClplM6aI+BLETFjm3ondmAcvYBDI+KNVmLJTdIkssR7WERslPQA28zWVCLSeV/b9s/ArC2+ZmnlzAA+n+bNRNK7JfUHHgROTdc0R5BNUrytWcCRksak7w5O5X8Fdimpdw/wpZYNSePT6oNkkxwj6QRgtwqxDgTWpkR5AFnLtkUvoKV1/Emy7v3rwEuSPp7OIUkHVTiH9WBOllbOVWTXI5+U9DTwS7LeyG3A/LTvWlqZZDgiVgNnk3V5/8zWbvAdwMdaBniALwMT0wDSs2wdlf9fZMn2GbLu+OIKsd4N9Jb0HPADsmTdYgNwSPoZJgPfTeWnA2el+J4B3vEKD7MWnnXIzCwHtyzNzHJwsjQzy8HJ0swsBydLM7McnCzNzHJwsjQzy8HJ0swsh/8PTsIat2liUBsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(forest_model, X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>is_next_day_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.163646</td>\n",
       "      <td>0.566594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.777627</td>\n",
       "      <td>0.308309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.188973</td>\n",
       "      <td>0.189404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017547</td>\n",
       "      <td>1.277594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.024903</td>\n",
       "      <td>1.393567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>0.082068</td>\n",
       "      <td>1.161210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>-0.671946</td>\n",
       "      <td>1.093416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1.325861</td>\n",
       "      <td>1.905336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0.734949</td>\n",
       "      <td>1.106097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>-0.336773</td>\n",
       "      <td>-1.238634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predictions  is_next_day_positive\n",
       "0       0.163646              0.566594\n",
       "1       0.777627              0.308309\n",
       "2      -0.188973              0.189404\n",
       "3       0.017547              1.277594\n",
       "4      -0.024903              1.393567\n",
       "..           ...                   ...\n",
       "577     0.082068              1.161210\n",
       "578    -0.671946              1.093416\n",
       "579     1.325861              1.905336\n",
       "580     0.734949              1.106097\n",
       "581    -0.336773             -1.238634\n",
       "\n",
       "[582 rows x 2 columns]"
      ]
     },
     "execution_count": 1370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XGBoost\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    learning_rate = 0.1,\n",
    "    max_depth = 8,\n",
    "    min_child_weight = 2,\n",
    "    gamma = 0.5,\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.8,\n",
    "    scale_pos_weight = 0.3\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "predictions = pd.DataFrame(y_pred)\n",
    "\n",
    "predictions = predictions.rename(columns={0: \"predictions\"})\n",
    "\n",
    "actual = pd.DataFrame(y_test)\n",
    "\n",
    "actual = actual.reset_index(drop = True)\n",
    "\n",
    "results = pd.concat([predictions, actual], axis=1)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6709152774061946\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(y_test, predictions, squared = False)\n",
    "\n",
    "print(np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network\n",
    "train_dataset = number_prediction.sample(frac=0.8, random_state=0)\n",
    "test_dataset = number_prediction.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('is_next_day_positive')\n",
    "test_labels = test_features.pop('is_next_day_positive')\n",
    "\n",
    "train_features = np.asarray(train_features).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example: [[ 1. 23.  6. ...  0.  0.  0.]\n",
      " [15. 24.  6. ...  0.  0.  0.]\n",
      " [ 3. 14.  4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 1. 14.  4. ...  0.  0.  0.]\n",
      " [10.  2.  1. ...  0.  0.  0.]\n",
      " [10. 50. 12. ...  0.  0.  0.]]\n",
      "\n",
      "Normalized: [[-1.68 -0.26 -0.17 ... -0.02 -0.02 -0.02]\n",
      " [-0.09 -0.19 -0.17 ... -0.02 -0.02 -0.02]\n",
      " [-1.46 -0.88 -0.78 ... -0.02 -0.02 -0.02]\n",
      " ...\n",
      " [-1.68 -0.88 -0.78 ... -0.02 -0.02 -0.02]\n",
      " [-0.66 -1.72 -1.69 ... -0.02 -0.02 -0.02]\n",
      " [-0.66  1.61  1.64 ... -0.02 -0.02 -0.02]]\n"
     ]
    }
   ],
   "source": [
    "normalizer = preprocessing.Normalization()\n",
    "\n",
    "normalizer.adapt(np.array(train_features))\n",
    "\n",
    "first = np.array(train_features)\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "  print('First example:', first)\n",
    "  print()\n",
    "  print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_36 (Normaliza  (None, 58)               117       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                3776      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,118\n",
      "Trainable params: 8,001\n",
      "Non-trainable params: 117\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "features_array = np.array(train_features)\n",
    "\n",
    "normalizer = preprocessing.Normalization(input_shape=[1,])\n",
    "normalizer.adapt(features_array)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "     layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='normalization_36_input'), name='normalization_36_input', description=\"created by layer 'normalization_36_input'\"), but it was called on an input with incompatible shape (None, 58).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='normalization_36_input'), name='normalization_36_input', description=\"created by layer 'normalization_36_input'\"), but it was called on an input with incompatible shape (None, 58).\n",
      "52/59 [=========================>....] - ETA: 0s - loss: 1.9353WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='normalization_36_input'), name='normalization_36_input', description=\"created by layer 'normalization_36_input'\"), but it was called on an input with incompatible shape (None, 58).\n",
      "59/59 [==============================] - 1s 5ms/step - loss: 1.9479 - val_loss: 2.3234\n",
      "Epoch 2/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.8840 - val_loss: 2.2972\n",
      "Epoch 3/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.8480 - val_loss: 2.2808\n",
      "Epoch 4/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.8223 - val_loss: 2.2798\n",
      "Epoch 5/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.8004 - val_loss: 2.2913\n",
      "Epoch 6/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.7814 - val_loss: 2.2752\n",
      "Epoch 7/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.7578 - val_loss: 2.2678\n",
      "Epoch 8/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.7359 - val_loss: 2.2793\n",
      "Epoch 9/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.7251 - val_loss: 2.2629\n",
      "Epoch 10/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.6937 - val_loss: 2.2654\n",
      "Epoch 11/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.6881 - val_loss: 2.2912\n",
      "Epoch 12/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.6711 - val_loss: 2.2834\n",
      "Epoch 13/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.6376 - val_loss: 2.2783\n",
      "Epoch 14/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.6169 - val_loss: 2.2782\n",
      "Epoch 15/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.5951 - val_loss: 2.2807\n",
      "Epoch 16/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.5840 - val_loss: 2.3058\n",
      "Epoch 17/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.5662 - val_loss: 2.2951\n",
      "Epoch 18/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.5386 - val_loss: 2.2941\n",
      "Epoch 19/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.5216 - val_loss: 2.3037\n",
      "Epoch 20/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.5066 - val_loss: 2.2903\n",
      "Epoch 21/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.5018 - val_loss: 2.3119\n",
      "Epoch 22/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.4774 - val_loss: 2.2879\n",
      "Epoch 23/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.4615 - val_loss: 2.3188\n",
      "Epoch 24/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.4365 - val_loss: 2.3420\n",
      "Epoch 25/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.4241 - val_loss: 2.3042\n",
      "Epoch 26/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.3933 - val_loss: 2.3146\n",
      "Epoch 27/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.3914 - val_loss: 2.3211\n",
      "Epoch 28/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.3747 - val_loss: 2.3163\n",
      "Epoch 29/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.3677 - val_loss: 2.3308\n",
      "Epoch 30/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.3499 - val_loss: 2.3268\n",
      "Epoch 31/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.3270 - val_loss: 2.3159\n",
      "Epoch 32/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.3180 - val_loss: 2.3283\n",
      "Epoch 33/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.2936 - val_loss: 2.3438\n",
      "Epoch 34/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.2732 - val_loss: 2.3444\n",
      "Epoch 35/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.2755 - val_loss: 2.3528\n",
      "Epoch 36/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.2423 - val_loss: 2.3449\n",
      "Epoch 37/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.2234 - val_loss: 2.3328\n",
      "Epoch 38/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.2078 - val_loss: 2.3857\n",
      "Epoch 39/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.1992 - val_loss: 2.3531\n",
      "Epoch 40/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1806 - val_loss: 2.3651\n",
      "Epoch 41/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1813 - val_loss: 2.3687\n",
      "Epoch 42/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1524 - val_loss: 2.3659\n",
      "Epoch 43/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1519 - val_loss: 2.3599\n",
      "Epoch 44/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1240 - val_loss: 2.3785\n",
      "Epoch 45/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1361 - val_loss: 2.3653\n",
      "Epoch 46/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1352 - val_loss: 2.3666\n",
      "Epoch 47/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.1180 - val_loss: 2.4137\n",
      "Epoch 48/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0941 - val_loss: 2.3793\n",
      "Epoch 49/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0880 - val_loss: 2.4380\n",
      "Epoch 50/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0660 - val_loss: 2.3871\n",
      "Epoch 51/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.0729 - val_loss: 2.4125\n",
      "Epoch 52/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.0530 - val_loss: 2.4073\n",
      "Epoch 53/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 1.0562 - val_loss: 2.4105\n",
      "Epoch 54/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0468 - val_loss: 2.4035\n",
      "Epoch 55/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0507 - val_loss: 2.4059\n",
      "Epoch 56/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0185 - val_loss: 2.4229\n",
      "Epoch 57/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0206 - val_loss: 2.4387\n",
      "Epoch 58/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 1.0003 - val_loss: 2.4354\n",
      "Epoch 59/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9833 - val_loss: 2.4124\n",
      "Epoch 60/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 2.4270\n",
      "Epoch 61/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9709 - val_loss: 2.4407\n",
      "Epoch 62/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9633 - val_loss: 2.4432\n",
      "Epoch 63/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.9566 - val_loss: 2.4335\n",
      "Epoch 64/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9768 - val_loss: 2.4440\n",
      "Epoch 65/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9436 - val_loss: 2.4391\n",
      "Epoch 66/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9329 - val_loss: 2.4318\n",
      "Epoch 67/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9388 - val_loss: 2.4289\n",
      "Epoch 68/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.9293 - val_loss: 2.4423\n",
      "Epoch 69/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9181 - val_loss: 2.4589\n",
      "Epoch 70/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.9292 - val_loss: 2.4660\n",
      "Epoch 71/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.9067 - val_loss: 2.4453\n",
      "Epoch 72/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.9145 - val_loss: 2.4691\n",
      "Epoch 73/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.8953 - val_loss: 2.4632\n",
      "Epoch 74/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8910 - val_loss: 2.4854\n",
      "Epoch 75/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.8787 - val_loss: 2.4470\n",
      "Epoch 76/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8694 - val_loss: 2.4742\n",
      "Epoch 77/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.8629 - val_loss: 2.4877\n",
      "Epoch 78/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8563 - val_loss: 2.4830\n",
      "Epoch 79/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8573 - val_loss: 2.4811\n",
      "Epoch 80/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8539 - val_loss: 2.4787\n",
      "Epoch 81/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8532 - val_loss: 2.4958\n",
      "Epoch 82/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8343 - val_loss: 2.5005\n",
      "Epoch 83/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8257 - val_loss: 2.4693\n",
      "Epoch 84/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8298 - val_loss: 2.5208\n",
      "Epoch 85/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8141 - val_loss: 2.5205\n",
      "Epoch 86/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.8241 - val_loss: 2.5046\n",
      "Epoch 87/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8248 - val_loss: 2.5343\n",
      "Epoch 88/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8097 - val_loss: 2.5126\n",
      "Epoch 89/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.7951 - val_loss: 2.5489\n",
      "Epoch 90/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.8113 - val_loss: 2.5397\n",
      "Epoch 91/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.7905 - val_loss: 2.5503\n",
      "Epoch 92/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 2.5352\n",
      "Epoch 93/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.7856 - val_loss: 2.5176\n",
      "Epoch 94/100\n",
      "59/59 [==============================] - 0s 4ms/step - loss: 0.7628 - val_loss: 2.5421\n",
      "Epoch 95/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.7630 - val_loss: 2.5183\n",
      "Epoch 96/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.7670 - val_loss: 2.5295\n",
      "Epoch 97/100\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.7491 - val_loss: 2.5505\n",
      "Epoch 98/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.7711 - val_loss: 2.5200\n",
      "Epoch 99/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.7765 - val_loss: 2.5483\n",
      "Epoch 100/100\n",
      "59/59 [==============================] - 0s 3ms/step - loss: 0.7470 - val_loss: 2.5359\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=100,\n",
    "    # suppress logging\n",
    "    verbose=1,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.767003</td>\n",
       "      <td>2.529469</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.749054</td>\n",
       "      <td>2.550456</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.771062</td>\n",
       "      <td>2.520028</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.776497</td>\n",
       "      <td>2.548292</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.746987</td>\n",
       "      <td>2.535909</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  val_loss  epoch\n",
       "95  0.767003  2.529469     95\n",
       "96  0.749054  2.550456     96\n",
       "97  0.771062  2.520028     97\n",
       "98  0.776497  2.548292     98\n",
       "99  0.746987  2.535909     99"
      ]
     },
     "execution_count": 1377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81ce073980d74db8b40d5b7dfd6daf3a2c2bbd872f27ac6d9cd03d097aee0b55"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
